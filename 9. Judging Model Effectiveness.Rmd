---
title: "9. Judging Model Effectiveness"
author: "Russ Conte"
date: '2022-05-27'
output: html_document
---
# Judging Model Effectiveness

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Start by loading the required packages in two lines of code:

```{r Load all required packages in two lines of code}

Packages <- c("tidymodels", "caret", "multilevelmod", "lme4", "VCA", "survival", "patchwork", "splines")
lapply(Packages, library, character = TRUE)

tidymodels_prefer(quiet = FALSE)

```

Start with code for multi-core processing

```{r multi-core processing}

library(doParallel)
cl <- makePSOCKcluster(10)
registerDoParallel(cl)

```

```{r Code for this chapter, from previous chapters}

library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)
  
lm_model <- linear_reg() %>% set_engine("lm")

lm_workflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_workflow, ames_train)

```

Once we have a model, we need to know how well it works. A quantitative approach for estimating effectiveness allows us to understand the model, to compare different models, or to tweak the model to improve performance. Our focus in tidy models is an empirical validation; this usually means using data that were not used to create the model as the substrate to measure effectiveness.

The best approach to empirical validation involves using *resampling* methods that will be introduced in chapter 10. In this chapter, we motivate the need for empirical validation by using the test set. Keep in mind that the tests that can only be used once, is explained in section 5.1.

## 9.1 Performance Metrics and Inference

The effectiveness of any given model depends on how the model will be used. An inferential model is used primarily to understand relationships, and typically emphasizes the choice (and validity) of probabilistic distributions and other generative qualities that define the model.

For a model used primarily for prediction, by contrast, predictive strength is of primary importance and other concerns about underlying statistical qualities may be less important. Predictive strength is usually determined by how close our predictions come to the observed data, i.e. fidelity of the model predictions to the actual results. This chapter focuses on functions that can be used to measure predictive strength. However, our advice for those developing inferential models is to use these techniques even when the model will not be used with the primary goal of prediction.

Using resampling methods, discussed in chapter 10, we can estimate the accuracy of this model to be about 72.7%. **Accuracy is often a poor measure of model performance**; we use it here because it is commonly understood.

In the remainder of the chapter, we will discuss general approaches for evaluating models by empirical validation. These approaches are grouped by the nature of the outcome data: purely numeric, binary classes, and three or more class levels.

## 9.2 Regression Metrics

To illustrate regression metrics, let's check the model from section 8.8. This model `lm_workflow_fit` combines a linear regression model with a predictor set supplemented with an interaction and spline functions for longitude and latitude. It was created from a training set (named `Ames_train`). Although we do not advise using the test site at this juncture of the modeling process, it will be used here to illustrate functionality and syntax. The data frame `Ames_test` consists of 588 properties. To start, let's produce predictions:

```{r Produce predictions (not recommended to use the test set at this step of the analysis)}

ames_test_res <- predict(lm_fit, new_data = ames_test %>%  select(-Sale_Price))
ames_test_res

```

The predicted numeric outcome from the regression model is names `.pred`. Let's match the predicted values with their corresponding observed outcome values:

```{r Match observed vs predicted outcome values}

ames_test_res <- bind_cols(ames_test_res, ames_test %>% select(Sale_Price))
ames_test_res

```

We see that these values mostly look close, but we don't yet have a quantitative understanding of how the model is doing because we haven't computed any performance metrics. Note that both the predicted and observed outcomes are in log-10 units. It is best practice to analyze the predictions on the transformed scale (if one were used) even if the predictions are reported using original units.

Let's plot the data in figure 9.2 before computing metrics:

```{r Plotting predicted vs actual sale price of the log of the data}

ggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) +
  # Create a diagonal line
  geom_abline(lty = 2) +
  geom_point(alpha = 0.5) +
  labs(y = "Predicted Sale Price (log10)", x = "Sale Price (log10)") +
  # Scale and size the x- and y- axes uniformly:
  coord_obs_pred()

```

Let's compute the root mean squared error for this model using the `rmse()` function:

```{r Calculate the root mean squared error for this function}

rmse(ames_test_res, truth = Sale_Price, estimate = .pred)

```

This shows us the standard format of the output of **yardstick** functions. Metrics for numeric outcomes usually have a "standard" for the `.estimator` column. Examples with different values for this column are shown in the next sections.

To compute multiple metrics at once, we can create a *metric set*. Let's add $R^2$ and the mean absolute error:

```{r Create a metric set}

ames_metrics <- metric_set(rmse, rsq, mae)
ames_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)

```

This tidy data format stacks the metrics vertically. The route means squared error and mean absolute air metrics are both on the scale of the outcome (so back tick back tick for our example) and measure the difference between the predicted and observed values. The value for $R^2$ Measures the squared correlation between the predicted and observed values, so values closer to one are better.

## 9.3 Binary Classification Metrics

To illustrate other ways to measure model performance, we will switch to a different example. The **modeldata** package (another one of the tidymodels packages) contains example predictions from a test data set with two classes ("Class 1" and "Class 2"):

```{r Two class example}

data("two_class_example")
tibble(two_class_example) %>% 
  map_dbl(mean)
 
```

For the hard class predictions, a variety of **yardstick** functions are helpful:

Confusion matrix:

```{r Yardstick functions for hard class predictions}

conf_mat(two_class_example, truth = truth, estimate = predicted)

```

Accuracy:

```{r Accuracy}

accuracy(two_class_example, truth, predicted)

```

Matthew's Correlation Coefficient:

```{r Matthews Correlation Coefficient}

mcc(two_class_example, truth, predicted)

```

F1 metric:

```{r F1 metric}

f_meas(two_class_example, truth, predicted)

```

Combining these three classification metrics together:

```{r Combining three classification metrics together}

classification_metrics <- metric_set(accuracy, mcc, f_meas)
classification_metrics(two_class_example, truth = truth, estimate = predicted)

```

The Matthews correlation coefficient and F1 score both summarize the confusion matrix, but compared to `mcc()`, which measures the quality of both positive and negative examples, the `f_meas()` metric emphasizes the positive class, i.e., the event of interest. For binary classification data sets like this example, yardstick functions have a standard argument called `event_level` to distinguish positive and negative levels. The default (which we used in this code) is that the first level of the outcome factor is the event of interest.

For example, the receiver operating characteristic (ROC) curve computes the sensitivity and specificity over a continuum of different event thresholds. The predicted class column is not used. There are two **yardstick** functions for this method: `roc_curve()` computes the data points that make up the ROC curve and `roc_auc()` computes the area under the curve.

The interfaces to these types of metric functions use the `...` argument placeholder to pass in the appropriate class probability column. For two-class problems, the probability column for the event of interest is passed into the function:

```{r ROC curve}

two_class_curve <- roc_curve(two_class_example, truth, Class1)
two_class_curve


```

Let's estimate the ROC curve:

```{r Look at the ROC curve}

roc_auc(two_class_example, truth, Class1)

```

Let's look at the ROC curve:

```{r}

autoplot(two_class_curve)
```

If the curve was close to the diagonal line, then the models predictions would be no better than random guessing. Since the curve is in the top, left – hand corner, we see that our model performs well at different thresholds.

There are a number of other functions that use probability estimates, including `gain_curve()`, `lift_curve()`, and `pr_curve()`

## 9.4 Multiclass Classification Metrics

What about data with three or more classes? To demonstrate, let's explore a different example data set that has four classes:

```{r Looking at a data set with four classes}

data("hpc_cv")
tibble(hpc_cv)

```

```{r look at the mean of the numeric columns}

hpc_cv %>% 
  select(where(is.numeric)) %>% 
  map_dbl(mean)

```

The functions for metrics that use the discrete class predictions are identical to their binary counterparts:

```{r Metrics for discrete class predictions}

accuracy(hpc_cv, obs, pred)

mcc(hpc_cv, obs, pred)

```

There are wrapper methods that can be used to apply sensitivity, the true positive rate, to our for class outcome. These options are macro-averaging, macro-weighted average in, and micro-averaging:

* Macro-averaging computes a set of one-versus-all metrics using the standard to class statistics. These are averaged.

* Macro – waited averaging does the same but the average is waited by the number of samples in each class

* Micro – averaging computes the contribution for each class, aggregates them, then compute a single metric from the aggregates.

**yardstick** can automatically apply these methods via the `estimator` argument:

```{r Using yardstick to automatically apply metrics to the data set}

sens(hpc_cv, obs, pred, estimator = "macro")

sens(hpc_cv, obs, pred, estimator = "macro_weighted")

sens(hpc_cv, obs, pred, estimator = "micro")



```

Note - there is another typo in the text, the function is `sens`, but the book has it as `sensitivity`.

```{r all of the metrics can be computed using **dplyr** groupings}

hpc_cv %>% 
  group_by(Resample) %>% 
  accuracy(obs, pred)

```

The groupings also translate to the `autoplot()` methods with results shown in figure 9.4:

```{r Using the autoplot function}

hpc_cv %>% 
  group_by(Resample) %>% 
  roc_curve(obs, VF, F, M, L) %>% 
  autoplot()

```

This visualization shows us that the different groups all perform about the same, but that the `VF` class is predicted better than th `F` or `M` classes, since the `VF` ROC curves are more in the top-left corner. This example uses resamples as the groups, but any grouping in your data can be used. This `autoplot()` method can be a quick visualization method for model effectiveness across outcome classes and/or groups.

## 9.5 Chapter Summary

Different metrics measure different aspects of a model fit, RMSE measures accuracy, while the $R^2$ Measures correlation. Measuring model performance is important even when a given model will not be used primarily for production; predictive power is also important for inferential or descriptive models.

Functions from the**yardstick**package special the effectiveness of a model using data. The prime tidy model interface uses tidy verse principles and data frames (as supposed to having vector arguments). Different metrics are appropriate for regression and classification metrics and, within these, there are sometimes different ways to estimate the statistics, such as for multi class outcomes.

REFERENCES

Craig–Schapiro, R, M Kuhn, C Xiong, E Pickering, J Liu, T Misko, R Perrin, et al. 2011. “Multiplexed Immunoassay Panel Identifies Novel CSF Biomarkers for Alzheimer’s Disease Diagnosis and Prognosis.” PLoS ONE 6 (4): e18850.
Hand, D, and R Till. 2001. “A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems.” Machine Learning 45 (August): 171–86.
Hosmer, D, and Sy Lemeshow. 2000. Applied Logistic Regression. New York: John Wiley; Sons.
Jungsu, K, D Basak, and D Holtzman. 2009. “The Role of Apolipoprotein E in Alzheimer’s Disease.” Neuron 63 (3): 287–303.
Opitz, J, and S Burst. 2019. “Macro F1 and Macro F1.” https://arxiv.org/abs/1911.03347.
Wu, X, and Z Zhou. 2017. “A Unified View of Multi-Label Performance Measures.” In International Conference on Machine Learning, 3780–88.