---
title: "1. Comparing Models with Resampling"
author: "Russ Conte"
date: '2022-05-27'
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Start by loading the required packages in two lines of code:

```{r Load all required packages in two lines of code}

Packages <- c("tidymodels", "caret", "multilevelmod", "lme4", "VCA", "survival", "patchwork", "splines", "ranger", "ggrepel", "corrr", "tidyposterior", "rstanarm")
lapply(Packages, library, character = TRUE)

tidymodels_prefer(quiet = FALSE)

```

Start with code for multi-core processing

```{r multi-core processing}

library(doParallel)
cl <- makePSOCKcluster(10)
registerDoParallel(cl)

```

```{r Code for this chapter, from previous chapters}

library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)
  
lm_model <- linear_reg() %>% set_engine("lm")

lm_workflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_workflow, ames_train)

set.seed(1001)
ames_folds <- vfold_cv(ames_train, v = 10)

rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_workflow <- 
  workflow() %>% 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude + Longitude) %>% 
  add_model(rf_model)

keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

set.seed(1003)
rf_res <- 
  rf_workflow %>% 
  fit_resamples(resamples = ames_folds, control = keep_pred)

```

## 11.1 Creating Multiple Models with Workflow Sets

In section 7.5 we described the idea of a workflow set or different pre-processors and or models can combinatorially generated. In chapter 10, we used a recipe for the Ames data that included an interaction term as well as spline functions for longitude and latitude. To demonstrate more with workflow sets, let's create three different linear models that add these pre-processing steps incrementally. We can test whether these additional terms improve the model results. We will create three recipes, then combine them into a workflow set:

```{r Create three recipes, then combine into a workflow set}

basic_recipe <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude + Longitude, data = ames_train) %>% 
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors())

interaction_recipe <- 
  basic_recipe %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_"))

spline_recipe <- 
  interaction_recipe %>% 
  step_ns(Latitude, Longitude, deg_free = 50)

preproc <- 
  list(
    basic = basic_recipe,
    interact = interaction_recipe,
    splines = spline_recipe
  )

lm_models <- workflow_set(preproc, list(lm = lm_model), cross = FALSE)

lm_models

keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

```

We'd like to re-sample each of these models in turn. To do so, we will use a **purrr**-like function called `workflow_map ()`. This function takes an initial argument of the function to apply to the workflows, followed by options to that function. We also set a `verbose` argument that will print the progress as well as a `seed` argument that makes sure that each model uses the same random number stream as the others.

```{r Resample each of the models in turn}

lm_models <- 
  lm_models %>% 
  workflow_map("fit_resamples",
               # Options to `workflow_map()`:
               seed = 1101,
               verbose = TRUE,
               # Options to `fit_resamples()`:
               resamples = ames_folds,
               control = keep_pred)

```

Let's look at lm_models

```{r Look at lm_models}

lm_models

```

Noticed that the `option` and `result` columns are now populated. The former includes the options to `fit_resamples()` that were given (for reproducibility), and the latter column contains the results produced by `fit_resamples()`.

There are a few convenience functions for workflow sets, including `collect_metrics()` to collate the performance statistics. We can `filter` to any specific metric we are interested in:

```{r Filtering the workflow for any specific metrics}

collect_metrics(lm_models) %>% 
  filter(.metric == "rmse")

```

What about the random Forrest model from the previous chapter? We can add it to the set by first converting its own workflow set, then binding rows. This requires that, when the model was resampled, the `save_workflow=TRUE` option was set in the control function.

```{r Adding the random forest model}

four_models <- 
  as_workflow_set(random_forest = rf_res) %>% 
  bind_rows(lm_models)
four_models

```

How does random forest compare to the previous three models?

```{r How does random forest compare to the previous three models?}

collect_metrics(four_models) %>% 
  filter(.metric == "rmse") %>% 
  arrange(mean)
```

Note that random forest has the lowest of the four in root-mean-squared error

The `autoplot()` method, with output in figure 11.1, shows confidence intervals for each model in order of best to worst. In this chapter, we will focus on the coefficients of determination (a.k.a. $R^2$) and use `metric = "rsq"` in the call to set up our plat:

```{r Plotting r-squared for each of the four models}

autoplot(four_models, metric = "rsq") +
  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")

```

From this plot of $R^2$ confidence intervals, we can see that the random forest method is doing the best job and there are minor improvements in the linear models as we add more recipe steps.

Now that we have ten resampled performance estimates for each of the four models, these summary statistics can be used to make between-model comparisons.

## 11.2 Comparing Resampled Performance Statistics

Considering the preceding results for the three linear models, it appears that the additional terms do not profoundly improve the mean RMSE or $R^2$ statistics for the linear models. The difference is small, but it might be larger than the experimental noise in the system, i.e., considered statistically significant. We can formally test the hypothesis that the additional terms increase $R^2$.

In other words, there are some re-samples where performance across models tends to be low and others where it tends to be high. In statistics this is called a *resample-to-resample* component of variation.

To illustrate, let's gather the individual resampling statistics for the linear models and the random forest. Will focus on the $R^2$ statistic for each model, which measures correlation between observed and predicted sales prices for each house. Let's `filter ()` to keep only the $R^2$ metrics, reshape the results, and compute how the metrics are correlated with each other.

```{r Gather individual resampling statistics for the linear models, keep the R^2 metrics}

rsq_individual_estimates <- 
  collect_metrics(four_models, summarize = FALSE) %>% 
  filter(.metric == "rsq")

rsq_wider <- 
  rsq_individual_estimates %>% 
  select(wflow_id, .estimate, id) %>% 
  pivot_wider(id_cols = "id", names_from = "wflow_id", values_from = 
                .estimate)

corrr::correlate(rsq_wider %>%  select(-id), quiet = TRUE)

```

Let's see this visually:

```{r Observe correlations between models visually}

rsq_individual_estimates %>% 
  mutate(wflow_id = reorder(wflow_id, .estimate)) %>% 
  ggplot(aes(x = wflow_id, y = .estimate, group = id, color = id)) +
  geom_line(alpha = 0.5, lwd = 1.25) +
  theme(legend.position = "none") +
  ggtitle("Figure 11.2: Resample statistics across models") +
  ylim(0, 1)

```

If the resample–to-resample effect was not real, there would not be any parallel lines. A statistical test for the correlations evaluates whether the magnitudes of these correlations are not simply noise. For the linear models:

```{r testing if the magnitudes of the correlations is noise or effect}

rsq_wider %>% 
  with(cor.test(basic_lm, splines_lm)) %>% 
  tidy() %>% 
  select(estimate, starts_with("conf"))

```

The results of the correlation test (the `estimate` of the correlation and confidence intervals) show us that they within–resample correlations appear to be real.

What effect does the extra correlation have on our analysis? Consider the variance of a difference of two variables:

<center>$Var[X-Y] = Var[X] + Var{y} - 2Cov[X,Y]$</center>

The last term is the covariance between two items. If there is a significant positive covariance, then any statistical test of this difference would be critically under-powered comparing the difference in two models. In other words, ignoring the resample-to-resample effect would bias our model comparisons toward finding no differences between models.

Practical significance is subjective; two people can have very different ideas on the threshold for importance. However, we'll show later that this consideration can be very helpful when deciding between models.

## 11.3 Simple Hypothesis Testing Methods

We can use simple hypothesis testing to make formal comparisons between models. Consider the familiar linear statistical model:

<center>$y_{ij} = \beta_0 + \beta_1x_{i1} + ... + \beta_p x_{ip} + \epsilon_{ij}$</center>

This versatile model is used to create regression models as well as being the basis for the popular analysis of variance (ANOVA) technique for comparing groups. With the ANOVA model, the predictors ($x_{ij}$) are binary dummy variables for different groups. From this, the $\beta$ parameters estimate whether two or more groups are different from one another using hypothesis testing techniques.

In our specific situation, the ANOVA can also make model comparisons. Suppose the individual resamples $R^2$ statistics serve as the *outcome data* (i.e. the $y_{ij}) and the models as the *predictors* in the ANOVA model. A sampling of this data structure is shown in Table 11.1:

<center>Table 11.1: Model performance statistics as a data set for analysis</center>
||
|:---:|:---:|:---:|:---:|:---:|:---:|
|**Y = rsq**|**model**|**X1**|**X2**|**X3**|**id**|
|0.8108|basic_lm|0|0|0|Fold01|
|0.8134|interlace_lm|1|0|0|Fold01|
|0.8615|random_forest|0|1|0|Fold01|
|0.8217|splines_lm|0|0|1|Fold01|
|0.8045|basic_lm|0|0|0|Fold02|
|0.8103|interact_lm|1|0|0|Fold02|
---

The `X1`, `X2`, and `X3` columns in the table are indicators for the values in the `model` column. Their order was defined in the same way that R would define them, alphabetically ordered by `model`.

For our comparison, the specific ANOVA model is:

<center>$y_{ij} = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \epsilon_{ij}$</center>

where

* $\beta_0$ is the estimate of the mean $R^2$ when interactions are added to the basic linear model,
* $\beta_1$ is the change in mean $R^2$ when interactions are added to the basic linear model,
$ \beta_2 is the change in mean $R^2$ between the basic linear model and the random forest model,
* $\beta_3$ is the change in mean $R^2$ between the basic linear model and one with interactions and splines.

A simple and fast method for comparing two models at a time is to use the differences in $R^2$ values as the outcome data in the ANOVA model. Since the outcomes are matched by resample, the differences do not contain the resample-to-resample effect and, for this reason, the standard ANOVA model is appropriate. To illustrate, this call to `lm()` tests the difference between two of the linear regression models:

```{r Testing the difference between two linear regression models}

compare_lm <- 
  rsq_wider %>% 
  mutate(difference = splines_lm - basic_lm)

lm(difference ~ 1, data = compare_lm) %>% 
  tidy(conf.int = TRUE) %>% 
  select(estimate, p.value, starts_with("conf"))

rsq_wider %>% 
  with( t.test(splines_lm, basic_lm, paired = TRUE)) %>% 
  tidy() %>% 
  select(estimate, p.value, starts_with("conf"))

```

We could evaluate each pair-wise difference in this way. Note that the p-value indicates a *statistically significant* signal; the collection of spline terms for longitude and latitude do appear to have an effect. However, the difference in $R^2$ is estimated at 0.91%. If our practical effect size were 2%, we might not consider these terms worth including in the model.

11.4 ## Bayesian Methods

We just used hypothesis testing to formally compare models, but we can also take a more general approach to making these formal comparisons using random effects and Bayesian statistics (McElreath 2020). While the model is more complex than the ANOVA method, the interpretation is more simple and straight-forward than the p-value approach. The previous ANOVA model had the form:

<center>$y_{ij} = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \epsilon_{ij}$</center>

A Bayesian linear model makes additional assumptions. In addition to specifying a distribution for the residuals, we require *prior distribution* specifications for the model parameters ($\beta_j$ and $\sigma$). These are distributions for the parameters that the model assumes before being exposed to the observed data. For example, a simple set of prior distributions for our model might be:

<center>$\epsilon_{ij} \sim N(0, \sigma)\\
\beta_j \sim N(0, 10)\\
\sigma \sim \text{exponential(1)}$</center>

Given the observed data and the prior distribution specifications, the model parameters can then be estimated. The final distributions of the model parameters are combinations of the priors and the likelihood estimates. These *posterior distributions* of the parameters are the key distributions of interest. They are a full probabilistic description of the model's estimated parameters.

To adapt our Bayesian ANOVA model so that the resamples are adequately models, we consider a *random intercept model*. Here we assume that the samples impact the model only by changing the intercept. Note that this constrains the resamples from having a differential impact on the regression parameters $\beta_j$; these are assumed to have the same relationship across resamples. this model equation is:

<center>$y_{ij} = (\beta_0 + b_i) + \beta_1x_{i1} + \beta_2x_{i2} | \beta_3x_{i3} + \epsilon{}ij$</center>

This is not an unreasonable model for resampled statistics which, when plotted across models as in Figure 11.2, tend to have fairly parallel effects across models (i.e. little cross-over of lines).

The **tidyposterior** package has functions to fit Bayesian models for the purpose of comparing resamples models. The main function is called `perf_mod()` and it is configured to "just work" for different types of objects:

* For workflow sets, it creates an ANOVA model where the groups correspond to the workflows. Our exiting models did not optimize any tuning parameters (see the next three chapters). If one of the workflows in the set had data on turning parameters, the best tuning parameters set for each workflow is used in the Bayesian analysis. In other words, despite the presence of tuning parameters, `perf_mod()` focuses on making *between-workflow comparisons*.

* For objects that contain a single model that has been tuned using resampling, `perf_mod()` makes *within-model comparisons*. In this situation, the grouping variables tested in the Bayesian ANOVA model are the submodels defined by the tuning parameters.

* The `perf_mod()` function can also take a data frame produced by **rsample** that has columns of performance metrics associated with two or more model/workflow results. These could have been generated by nonstandard means.

For any of these types of objects, the `perf_mod()` function determines an appropriate Bayesian model and fits it with the resampling statistics. For our example, it will model the four sets of $R^2$ statistics associated with the workflows.

The **tidyposterior** package uses the **stan software** for specifying and fitting models via the **rstanarm** package. The functions within that package have default priors (see `?priors` for more details). The following model uses the default priors for all parameters except for the random intercepts (which follow a *t*-distribution). The estimation process uses random numbers so the seed is set within the function call. The estimation process is iterative and replicated several times in collections called *chains*. The `iter` parameter tells the function how long to run the estimation process in each chain. When several chains are used, their results are combined (assume that this is validated by diagnostic assessments).

```{r Using tidyposterior}

# The rstanarm package creates copious amounts of output; those results
# are not shown here but are worth inspecting for potential issues. The
# option `refresh = 0` can be used to eliminate the logging.

rsq_anova <- 
  perf_mod(
    four_models,
    metric = "rsq",
    prior_intercept = rstanarm::student_t(df = 1),
    chain = 4,
    iter = 5000,
    seed = 1102
  )

```

The resulting object has information on the resampling process as well as the Stan object embedded within (in an element called `stan`). We are most interested in the posterior distributions of the regression parameters. The **tidyposterior** package has a `tidy()` method that extracts these posterior distributions into a tibble:

```{r Extract posterior distributions into a tibble}

model_post <- 
  rsq_anova %>% 
  tidy(seed = 1103)

glimpse(model_post)

```

The four posterior distributions are visualized in Figure 11.3:

```{r Visualization of the four posterior distributions}

model_post %>% 
  mutate(model = forcats::fct_inorder(model)) %>% 
  ggplot(aes(x = posterior)) +
  geom_histogram(bins = 50, color = "white", fill = "blue", alpha = 0.4) +
  facet_wrap(~model, ncol = 1) +
  ggtitle("Figure 11.3: Posterior distributions for the coefficient of determination using four different models")

```

The histograms describe the estimated probability distributions of the mean $R^2$ value for each model. There is some overlap, especially for the three linear models.

There is also a basic `autoplot()` method for the model results, shown in Figure 11.4, as well as the tidied object that shows overlaid density plots.

```{r Using autoplot to show the results}

autoplot(rsq_anova) +
  geom_text_repel(aes(label = workflow), nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none") +
  ggtitle("Figure 11.4: Credible intervals derived from the model posterior distributions")

```

One wonderful aspect of using resampling with Bayesian models is that once w have the posteriors for the parameters, it is trivial to get the posterior distributions for combinations of the parameters. For example, to compare the two linear regression models, we are interested in the difference in means. The posterior of this difference is computed by sampling from the individual posteriors and taking the differences. The `contrast_models()` function can do this. To specify the comparisons to make, the `list_1` and `list_2 paramters take character vectors and computer the differences between the models in those lists (parameterized as `list_1 - list_2`).

We can compare two of the linear models and visualize the results in Figure 11.5:

```{r Visualize the differences in $$R^2$ between the models}

rqs_diff <- 
  contrast_models(rsq_anova,
                  list_1 = "splines_lm",
                  list_2 = "basic_lm",
                  seed = 1104)

rqs_diff %>% 
  as_tibble() %>% 
  ggplot(aes(x = difference)) +
  geom_vline(xintercept = 0, lty = 2) +
  geom_histogram(bins = 50, color = "white", fill = "red", alpha = 0.4) +
  ggtitle("Figure 11.5, Posterior distribution for the difference in the coefficient of determination")

```

The posterior shows that the center of the distribution is greater than zero (indicating that the model with splines typically had larger values) but does overlap with zero to a degree. The `summary()` method for this object computes the mean of the distribution as well as credible intervals, the Bayesian analog to confidence intervals.

```{r Calculating the summary}

summary(rqs_diff) %>% 
  select(-starts_with("pract"))

```

The `probability` column reflect the proportion of the posterior that is greater than zero. This is the probability that the positive difference is real. The value is not close to zero, providing a strong case for statistical significance, i.e. the idea that statistically the actual difference is not zero.

### The effect of the Amount of Resampling

How does the number of resamples affect these types of formal Bayesian comparisons? More resamples increases the precision of the overall resampling estimate; that precision propagates to this type of analysis. For illustration, additional resamples were added using repeated cross-validation. How did the posterior distribution change? Figure 11.7 shows the 90% credible intervals with up to 100 resamples (generated from 10 repeats of 10-fold cross-validation).

The code to generate intervals is available at <https://github.com/tidymodels/TMwR/blob/main/extras/ames_posterior_intervals.R> and it will take a long time to run.

## 11.5 Chapter Summary

This chapter described formal statistical methods for testing differences in performance between models. We demonstrated the within-resample effect, where results for the same resample tend to be similar; this aspect of resampled summary statistics requires appropriate analysis in order for valid model comparisons. Further, although statistical significance and practical significance are both important concepts for model comparisons, they are different.

REFERENCES

Faraway, J. 2016. Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models. CRC press.
Kruschke, J, and T Liddell. 2018. “The Bayesian New Statistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power Analysis from a Bayesian Perspective.” Psychonomic Bulletin and Review 25 (1): 178–206.
McElreath, R. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. CRC press.
Wasserstein, R, and N Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and pPurpose.” The American Statistician 70 (2): 129–33.