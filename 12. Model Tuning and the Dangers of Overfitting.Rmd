---
title: "12 Model Tuning and the Dangers of Overfitting"
author: "Russ Conte"
date: '2022-05-28'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Start by loading the required packages in two lines of code:

```{r Load all required packages in two lines of code}

Packages <- c("tidymodels", "caret", "multilevelmod", "lme4", "VCA", "survival", "patchwork", "splines", "ranger", "ggrepel", "corrr", "tidyposterior", "rstanarm")
lapply(Packages, library, character = TRUE)

tidymodels_prefer(quiet = FALSE)

```

Start with code for multi-core processing

```{r multi-core processing}

library(doParallel)
cl <- makePSOCKcluster(10)
registerDoParallel(cl)

```

# 12. Model Tuning and the Dangers of Overfitting

In order to use a model for prediction, the parameters for that model must be estimated. Some of these parameters can be estimated directly from the training data, but other parameters, called tuning parameters or hyperparameters, must be specified ahead of time and can’t be directly found from training data. These are unknown structural or other kind of values that have significant impact on the model but cannot be directly estimated from the data. This chapter will provide examples of tuning parameters and show how we use tidymodels functions to create and handle tuning parameters. We’ll also demonstrate how poor choices of these values lead to overfitting and introduce several tactics for finding optimal tuning parameters values. Chapters 13 and 14 go into more detail on specific optimization methods for tuning.

## 12.1 Model Paramters

In ordinary linear regression, there are two parameters, $\beta_0$ and $\beta_1$ of the model:
<center>$y_i = \beta_0 + \beta_1x_i + \epsilon_i$</center>

When we have the outcome ($y$) and predictor ($x$) data, we can estimate the two parameters $\beta_0$ and $\beta_1$:

<center>$\hat{\beta_1} = \frac{\sum(y_i = \bar y)(x_i - \bar x)}{\sum_i(x_i = \bar x)^2}$</center>

and

<center>$\hat{\beta_0} = \bar y - \hat{\beta_1} \bar x$</center>

We can directly estimate these values from the data for this example model because they are analytically tractable; if we have the data, then we can estimate these model parameters.

For the KNN model, the prediction equation for a new value $x_0$ is:

<center>$\hat{y} = \frac{1}{K}\sum_{\ell = 1}^K x_\ell^*$</center>

where K is the number of neighbors and $x_\ell^*$ are the $K$ closest values to $x_0$ in the training set. The model is not defined by a model equation; the previous prediction equation instead defines it.  This characteristic, along with the possible intractability of the distance measure, makes it impossible to create a set of equations that can be solved for $K$ (iteratively or otherwise). The number of neighbors has a profound impact on the model; it governs the flexibility of the class boundary. For small values of $K$, the boundary is very elaborate while for large values, it might be quite smooth.

The number of nearest neighbors is a good example of a tuning parameter or hyperparameter that cannot be directly estimated from the data.

## 12.2 Tuning Parameters for Different Types of Models

There are many examples of tuning parameters or hyperparameters in different statistical and machine learning models:

Boosting is an ensemble method that combines a series of base models, each of which is created sequentially and depends on the previous models. The number of boosting iterations is an important tuning parameter that usually requires optimization.

In the classic single-layer artificial neural network (a.k.a. the multilayer perceptron), the predictors are combined using two or more hidden units. The hidden units are linear combinations of the predictors that are captured in an activation function (typically a nonlinear function, such as a sigmoid). The hidden units are then connected to the outcome units; one outcome unit is used for regression models, and multiple outcome units are required for classification. The number of hidden units and the type of activation function are important structural tuning parameters.

Modern gradient descent methods are improved by finding the right optimization parameters. Examples of such hyperparameters are learning rates, momentum, and the number of optimization iterations/epochs (Goodfellow, Bengio, and Courville 2016). Neural networks and some ensemble models use gradient descent to estimate the model parameters. While the tuning parameters associated with gradient descent are not structural parameters, they often require tuning.

In some cases, preprocessing techniques require tuning:

In principal component analysis, or its supervised cousin called partial least squares, the predictors are replaced with new, artificial features that have better properties related to collinearity. The number of extracted components can be tuned.

Imputation methods estimate missing predictor values using the complete values of one or more predictors. One effective imputation tool uses $K$-nearest neighbors of the complete columns to predict the missing value. The number of neighbors modulates the amount of averaging and can be tuned.

Some classical statistical models also have structural parameters:

In binary regression, the logit link is commonly used (i.e., logistic regression). Other link functions, such as the probit and complementary log-log, are also available (Dobson 1999). This example is described in more detail in the Section 12.3.

Non-Bayesian longitudinal and repeated measures models require a specification for the covariance or correlation structure of the data. Options include compound symmetric (a.k.a. exchangeable), autoregressive, Toeplitz, and others (Littell, Pendergast, and Natarajan 2000).

A counterexample where it is inappropriate to tune a parameter is the prior distribution required for Bayesian analysis. The prior encapsulates the analyst’s belief about the distribution of a quantity before evidence or data are taken into account. For example, in Section 11.4, we used a Bayesian ANOVA model, and we were unclear about what the prior should be for the regression parameters (beyond being a symmetric distribution). We chose a t-distribution with one degree of freedom for the prior since it has heavier tails; this reflects our added uncertainty. Our prior beliefs should not be subject to optimization. Tuning parameters are typically optimized for performance whereas priors should not be tweaked to get “the right results.”

## 12.3 What Do We Optimize?

Each of these models results in linear class boundaries. Which one should we use? Since, for these data, the number of model parameters does not vary, the statistical approach is to compute the (log) likelihood for each model and determine the model with the largest value. Traditionally, the likelihood is computed using the same data that were used to estimate the parameters, not using approaches like data splitting or resampling from Chapters 5 and 10.

For a data frame `training_set`, let's create a function to compute the different models and extract the likelihood statistics for the training set (using `broom::glance()`):


```{r}

data("two_class_dat")
set.seed(91)
split <- initial_split(two_class_dat)
training_set <- training(split)
testing_set <- testing(split)

data_grid <- crossing(A = seq(0.4, 4, length = 200), B = seq(0.14, 3.9, length = 200))

llhood <- function(...){
  logistic_reg() %>% 
    set_engine("glm",...) %>% 
    fit(Class ~ ., data = training_set) %>% 
    glance() %>% 
    select(logLik)
}

bind_rows(
  llhood(),
  llhood(family = binomial(link = "probit")),
  llhood(family = binomial(link = "cloglog"))
) %>% 
  mutate(link = c("logit", "probit", "c-log-log"))  %>% 
  arrange(desc(logLik))
#> # A tibble: 3 × 2
#>   logLik link     
#>    <dbl> <chr>    
#> 1  -258. logit    
#> 2  -262. probit   
#> 3  -270. c-log-log
```


(note this is possibly a typo - R returns an error that "object 'training_set' not found, but the values match the text)

According to these results, the logistic model has the best statistical properties.

From the scale of the log-likelihood values, it is difficult to understand if these differences are important or negligible. One way of improving this analysis is to recompile the statistics and separate the modeling data fro the data used for performance estimation. With this small data set, repeated 10-fold cross-validation is a good choice for resampling. In the **yardstick** package, the `mn_log_loss()` function is used to estimate the negative log-likelihood, with our results shown in Figure 12.2:

```{r 10-fold cross validation, comparing loss across multiple models}

set.seed(1201)
rs <- vfold_cv(training_set, repeats = 10)

# Return the individual sampled performance estimates:

lloss <- function(...) {
  perf_meas <- metric_set(roc_auc, mn_log_loss)
  
  logistic_reg() %>% 
    set_engine("glm", ...) %>% 
    fit_resamples(Class ~ A + B, rs, metrics = perf_meas) %>% 
    collect_metrics(summarize = FALSE) %>% 
    select(id, id2, .metric, .estimate)
}

resampled_res <- 
  bind_rows(
    lloss()                                    %>% mutate(model= "logistic"),
    lloss(family = binomial(link = "probit"))  %>% mutate(model = "probit"),
    lloss(family = binomial(link = "cloglog")) %>% mutate(model = "c-log-log")
  ) %>% 
  mutate(.estimate = ifelse(.metric == "mn_log_loss", -.estimate, .estimate)) %>% 
  group_by(model, .metric) %>% 
  summarize(
    mean = mean(.estimate, na.rm = TRUE),
    std_err = sd(.estimate, na.rm = TRUE) / sum(!is.na(.estimate)),
    .groups = "drop"
  )


resampled_res %>% 
  filter(.metric == "mn_log_loss") %>% 
  ggplot(aes(x = mean, y = model)) +
  geom_point() +
  geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err), width = 0.1) +
  labs(y = NULL, x = "log-likelihood") +
  ggtitle("Figure 12.2: Means and approximate 90% confidence intervals for the resampled binomial log-likelihood with three different link functions")

```

These results show that there is considerable evidence that the choice of link function matters and that the logistic model is superior.

## 12.4 The Consequences of Poor Parameter Estimates

Many tuning parameters modulate the amount of model complexity. More complexity often implies more malleability in the patterns that a model can emulate. For example, as shown in Section 8.4.3, adding degrees of freedom in a spline function increases the intricacy of the prediction equation. While this is an advantage when the underlying motifs in the data are complex, it can also lead to over interpretation of chance patterns that would not reproduce in new data. Overfitting is the situation where a model adapts too much to the training data; it performs well for the data used to build the model but poorly for new data.

## 12.5 Two General Strategies for Optimization

Tuning parameter optimization usually falls into one of two categories: grid search and iterative search.

*Grid search* is when we pre-define a set of parameter values to evaluate. The main choices involved in cried search are how to make the grid and how many parameter combinations to evaluate. Great search as often judged as any efficient since the number of grid points required to cover the parameter space can become a manageable with the curse of dimensionality. There is truth to this concern, but it is most true when the process is not optimized. This is discussed more in chapter 13.

*Iterative search* Or sequential search is when we sequentially discover new parameter combinations based on previous results. Almost any nonlinear optimization method is appropriate, although summer more efficient than others. In some cases, and initial set of results for one or more parameter combinations is required to start the op optimization process. Iterative searches discussed more in chapter 14.

## 12.6 Tuning Parameters in Tinymodels

We've already dealt with quite a number of arguments that correspond to tuning parameters for recipe and model specifications in previous chapters. It is possible to tune:

* The threshold for combining neighborhoods into an "other" category (with the argument name `threshold`) discussed in Section 8.4.1

* The number of degrees of freedom in a natural spline (`deg_free`, Section 8.4.3)

* The number of data points required to execute a split in a tree-based model (`min_n`, section 6.1)

* The amount of regularization in penalized models (`penalty`, section 6.1)

For **parsnip** model specifications, there are two kinds of parameter arguments. *Main arguments* are those that are most often optimized for performance and available in multiple engines. The main tuning parameters are top-level arguments to the model specification function. For example, the `rand_forest()` function has main arguments `trees`, `min_n`, and `mtry` since these are most frequently specified or optimized.

A second set of tuning parameters are *engine specific*. These are either infrequently optimized or are specific only to certain engines. Again using random forests as an example, the **ranger** package contains some arguments that are not used by other packages. One example is gain penalization, which regularized the predictor selection in the tree induction process. This parameter can help modulate the trade-off between the number of predictors used in the ensemble and performance (Wundervald, Parnell, and Domijan 2020). The name of this argument in `ranger()` is `regularization.factor`. To specify a value via a **parsnip** model specification, it is added as a supplemental argument to `set_engine()`:

```{r Specifiying a value via parsnip model specification, this time in random forest}

rand_forest(trees = 2000, min_n = 10) %>% 
  set_engine("ranger", regularization_factor = 0.5)

```

How can we signal to tidy models functions which arguments should be optimized? Parameters are marked for tuning by assigning them a value of `tune()`. For the single layer neural network used in Section 12.4, the number of hidden units is designated for tuning:

From the github repo:

```{r tuning-two-class-comps, include = FALSE}
two_class_rec <-
 recipe(Class ~ ., data = two_class_dat) %>% 
 step_normalize(all_numeric_predictors()) 
mlp_mod <- 
 mlp(hidden_units = tune(), epochs = 1000) %>% 
 set_engine("nnet") %>%
 set_mode("classification")
mlp_wflow <- 
 workflow() %>% 
 add_recipe(two_class_rec) %>% 
 add_model(mlp_mod)
mlp_res <-
 tibble(
  hidden_units = 1:20,
  train = NA_real_,
  test = NA_real_,
  model = vector(mode = "list", length = 20)
 )
for(i in 1:nrow(mlp_res)) {
  set.seed(27)
 tmp_mod <-
  mlp_wflow %>% finalize_workflow(mlp_res %>% slice(i) %>% select(hidden_units)) %>%
  fit(training_set)
 mlp_res$train[i] <-
  roc_auc_vec(training_set$Class, predict(tmp_mod, training_set, type = "prob")$.pred_Class1)
 mlp_res$test[i]  <-
  roc_auc_vec(testing_set$Class, predict(tmp_mod, testing_set, type = "prob")$.pred_Class1)
 mlp_res$model[[i]] <- tmp_mod
}

```

```{r two-class-boundaries}
#| echo = FALSE, 
#| fig.height = 8,
#| fig.cap = "Class boundaries for three models with increasing numbers of hidden units. The boundaries are fit on the training set and shown for the training and test sets.",
#| fig.alt = "Class boundaries for three models with increasing numbers of hidden units. The boundaries are fit on the training set and shown for the training and test sets. After a single hidden unit, the boundaries become wildly complex. The test set plots show that the more complex models do not conform to the data that was not used to fit the model."
te_plot <- 
  mlp_res %>% 
  slice(c(1, 4, 20)) %>% 
  mutate(
    probs = map(model, ~ bind_cols(data_grid, predict(.x, data_grid, type = "prob")))
  ) %>% 
  dplyr::select(hidden_units, probs) %>% 
  unnest(cols = c(probs)) %>% 
  mutate(
    label = paste(format(hidden_units), "units"),
    label = ifelse(label == " 1 units", " 1 unit", label)
  ) %>% 
  ggplot(aes(x = A, y = B)) + 
  geom_point(data = testing_set, aes(color = Class, pch = Class), 
             alpha = 0.5, show.legend = FALSE) + 
  geom_contour(aes( z = .pred_Class1), breaks = 0.5, color = "black") + 
  scale_color_manual(values = c("#CC6677", "#88CCEE")) + 
  facet_wrap(~ label, nrow = 1) + 
  coord_equal() + 
  ggtitle("Test Set") + 
  labs(x = "Predictor A", y = "Predictor B")
tr_plot <- 
  mlp_res %>% 
  slice(c(1, 4, 20)) %>% 
  mutate(
    probs = map(model, ~ bind_cols(data_grid, predict(.x, data_grid, type = "prob")))
  ) %>% 
  dplyr::select(hidden_units, probs) %>% 
  unnest(cols = c(probs)) %>% 
  mutate(
    label = paste(format(hidden_units), "units"),
    label = ifelse(label == " 1 units", " 1 unit", label)
  ) %>% 
  ggplot(aes(x = A, y = B)) +
  geom_point(data = training_set, aes(color = Class, pch = Class), 
             alpha = 0.5, show.legend = FALSE) + 
  geom_contour(aes( z = .pred_Class1), breaks = 0.5, color = "black") + 
  scale_color_manual(values = c("#CC6677", "#88CCEE")) + 
  facet_wrap(~ label, nrow = 1) + 
  coord_equal() + 
  ggtitle("Training Set") + 
  labs(x = "Predictor A", y = "Predictor B")
tr_plot / te_plot
```

```{r Designating which arguments should be optimized}

neural_net_spec <- 
  mlp(hidden_units = tune()) %>% 
  set_engine("keras")

```

The results show a value of `nparam[+]`, indicating that the number of hidden units is a numeric parameter.

There is an optional identification argument that associates a name with the parameters. This can come in handy when the same kind of parameter is being tuned in different places. For example, with the Ames housing data from Section 10.6, the recipe encoded both longitude and latitude with spline functions. If we want to tune the two spline functions to potentially have different levels of smoothness, we call step_ns() twice, once for each predictor. To make the parameters identifiable, the identification argument can take any character string:

```{r Making individual parameters identifiable}

data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_recipe <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude + Longitude, data = ames_train) %>% 
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = tune()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(~ Gr_Liv_Area:starts_with("Bldg_type_")) %>% 
  step_ns(Longitude, deg_free = tune("longitude df")) %>% 
  step_ns(Latitude, deg_free = tune("latitude df"))

recipes_param <- extract_parameter_set_dials(ames_recipe)

recipes_param
```

Note that the `identifier` and `type` columns are not the same for both of the spline parameters.

When a parameter and model specification are combined using a workflow, both sets of parameters are shown:

```{r Combining a recipe and model specification in a workflow}

workflow_param <- 
  workflow() %>% 
  add_recipe(ames_recipe) %>% 
  add_model(neural_net_spec) %>% 
  extract_parameter_set_dials()
workflow_param

```

The **dials** package also has a convenience function for extracting a particular parameter object:

```{r To identify the parameter using the id value}

workflow_param %>% extract_parameter_dials("threshold")

```

In some cases, it is easy to have reasonable defaults for the range of possible values. In other cases, the parameter range is critical and cannot be assumed. The primary turning parameter for random forest models is the number of predictor columns that are randomly samples for each split in the tree, usually noted as `mtry()`. Without knowing the number of predictors, this parameter range cannot be preconfigured and requires finalization.

```{r random forest finalization example}

rf_spec <- 
  rand_forest(mtry = tune()) %>% 
  set_engine("ranger", regularization.factor = tune("regularization"))

rf_param <- extract_parameter_set_dials(rf_spec)
rf_param

```

Complete parameter objects have `[+]` in their summary; a value of `[?]` indicates that at least one end of the possible range is missing. There are two methods for handling this. The first is to use `update()`, to add a range based on what you know about the data dimensions:

```{r Updating missing parameters}

rf_param %>% 
  update(mtry = mtry(c(1,70)))

```

However, this approach might not work if a recipe is attached to a workflow that uses steps that either add or subtract columns. If those steps are not slated for tuning, the `finalize()` function can execute the recipe once to obtain the dimensions:

```{r Applying the finalize function}

pca_recipe <- 
  recipe(Sale_Price ~ ., data = ames_train) %>% 
  # Select the square-footage predictors and extract their PCA components:
  step_normalize(contains("SF")) %>% 
  # Select the number of components needed to capture 95% of the variance in the predictors
  step_pca(contains("SF"), threshold = 0.95)

updated_param <- 
  workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(pca_recipe) %>% 
  extract_parameter_set_dials() %>% 
  finalize(ames_train)
updated_param

updated_param %>% extract_parameter_dials("mtry")

```

When the recipe is prepared, the finalize() function learns to set the upper range of mtry to 74 predictors.

Additionally, the results of `extract_parameter_set_dials()` will include engine-specific parameters (if any). They are discovered in the same way as the main arguments and included in the parameter set. The **dials** package contains parameter functions for all potentially tunable engine-specific parameters:

```{r}

rf_param

# Collection of 2 parameters for tuning
# 
#      identifier                  type    object
#            mtry                  mtry nparam[?]
#  regularization regularization.factor nparam[+]
# 
# Model parameters needing finalization:
#    # Randomly Selected Predictors ('mtry')
# 
# See `?dials::finalize` or `?dials::update.parameters` for more information.

regularization_factor()


```

Finally, some tuning parameters are best associated with transformations. A good example of this is the penalty parameter associated with many regularized regression models. This parameter is nonnegative and it is common to vary its values in log units. The primary **dials** parameter object indicates that a transformation is used by default:

```{r penalty}

penalty()

```

This is important to know, especially when altering the range. Ne range values must be in the transformed units:

```{r New range values must be in the transformed units}

penalty(c(-1,0)) %>% value_sample(1000) %>% summary()

 #  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 # 0.1002  0.1796  0.3284  0.4007  0.5914  0.9957 

# incorrect:
penalty(c(0.1, 1.0)) %>% value_sample(1000) %>% summary()

```

The scale can be changed if desired with the `trans` argument. You can use natural units but the same range:

```{r}

penalty(trans = NULL, range = 10^c(-10,0))

```

12.7 CHAPTER SUMMARY

This chapter introduced the process of tuning model hyperparameters that cannot be directly estimated from the data. Tuning such parameters can lead to overfitting, often by allowing a model to grow overly complex, so using resampled data sets together with appropriate metrics for evaluation is important. There are two general strategies for determining the right values, grid search and iterative search, which we will explore in depth in the next two chapters. In tidymodels, the tune() function is used to identify parameters for optimization, and functions from the dials package can extract and interact with tuning parameters objects.

REFERENCES

Cybenko, G. 1989. “Approximation by Superpositions of a Sigmoidal Function.” Mathematics of Control, Signals and Systems 2 (4): 303–14.
Dobson, A. 1999. An Introduction to Generalized Linear Models. Chapman; Hall: Boca Raton.
———. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” Annals of Statistics 29 (5): 1189–1232.
Goodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning. MIT Press.
Littell, R, J Pendergast, and R Natarajan. 2000. “Modelling Covariance Structure in the Analysis of Repeated Measures Data.” Statistics in Medicine 19 (13): 1793–1819.
Olsson, D, and L Nelson. 1975. “The Nelder-Mead Simplex Procedure for Function Minimization.” Technometrics 17 (1): 45–51.
Thomas, R, and D Uminsky. 2020. “The Problem with Metrics Is a Fundamental Problem for AI.” https://arxiv.org/abs/2002.08512.
Wundervald, B, A Parnell, and K Domijan. 2020. “Generalizing Gain Penalization for Feature Selection in Tree-Based Models.” https://arxiv.org/abs/2006.07515.