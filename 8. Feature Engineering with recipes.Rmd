---
title: "8. Feature Engineering with recipes"
author: "Russ Conte"
date: '2022-05-26'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Start by loading the required packages in two lines of code:

```{r Load all required packages in two lines of code}

Packages <- c("tidymodels", "caret", "multilevelmod", "lme4", "VCA", "survival", "patchwork", "splines")
lapply(Packages, library, character = TRUE)

tidymodels_prefer(quiet = FALSE)

```

Start with code for multi-core processing

```{r multi-core processing}

library(doParallel)
cl <- makePSOCKcluster(10)
registerDoParallel(cl)

```


```{r Code we will use for modeling the Ames data moving forward}

library(tidymodels)
tidymodels_prefer()
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.8, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)

lm_model <- linear_reg() %>% set_engine("lm")

lm_model <- 
  linear_reg() %>% 
  set_engine("lm")

lm_workflow <- 
  workflow() %>% 
  add_model(lm_model)

lm_workflow <- 
  lm_workflow %>% 
  add_formula(Sale_Price ~ Longitude + Latitude)

lm_fit <- fit(lm_workflow, ames_train)

```

## Introduction to Feature Engineering with recipes

Feature engineering entails reformatting predictor values to make them easier from model to use effectively. This includes transformations and encoding of the data to best represent their important characteristics. Imagine that you have two predictors in a data set that can be more effectively represented in your model as a ratio; creating a new predictor from the ratio of the original two is a simple example of feature engineering.

Take the location of a property in Ames as a more involved example. There are a variety of ways that this spatial information can be exposed to a model, including neighborhood (qualitative measure), longitude/latitude, distance to the nearest school or Iowa State University, and so on.When choosing how to encode these data in modeling, we might choose an option we believe is most associated with the outcome. The original format of the data, for example numeric (e.g., distance) versus categorical (e.g. neighborhood), is also a driving factor in feature engineering choices.

Other examples of pre-processing to better features of modeling include:

*Correlations between predictors can be reduced by a feature extraction or the removal of some predictors.

*Once and predictors have missing values, they can be imputed using a sub – model.

*Models that use variance – type measures may benefit from coercing the distribution of some predictors to be symmetric by estimating a transformation.

Feature engineering and data processing can also involve reformatting that may be required by the model. Some models use geometric distance metrics and, consequently, numeric protectors should be centered and scaled so that they are all in the same units. Otherwise, the distance values will be biased by the scale of each column.

Note that appendix A contains a small table of recommended pre-processing techniques for different models.

In this chapter we introduce the **recipes**package that you can use to combine different feature engineering and pre-processing tasks into a single object and apply those transformations to different data sets. The **recipes** package is, like **parsnip** from models, one of the core tidymodels packages.

This chapter uses the Ames housing data and the R objects created in the book so far and summarized in section 7.7.

## 8.1 A Simple `recipe()` For the Ames Housing Data

In this section, we will focus on a small subset of the predictors available in the Ames housing data:

• The neighborhood (qualitative, with 29 neighborhoods in the training set)

• The gross above-grade living area (continuous, names `Gr_Liv_Area`)

* The year built(`Year_Built`)

* The type of building (`Bldg_Type` with values `OneFam` (n = 1,936), `TwoFmCon` (n = 50), `Duplex` (n = 88), `Twnhs` (n = 77), and `TwnhsE` (n = 191))

Suppose that an initial ordinary linear regression model were fit to these data. Recalling that, in Chapter 4, the prices were pre=logged, a standard call to `lm()` might look like:

```{r One example of a standard call to LM of a small subset of the Ames housing data}

lm(Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Year_Built + Bldg_Type, data = ames)

```

A recipe is also an object that defines a series of steps for data processing. Unlike the formula method inside a modeling function, the recipe defines the steps via `step_*()` functions without immediately executing them; it is only a specification of what should be done. Here is a recipe equivalent to the previous formula that builds on the code summary in section 5.5:

```{r Code example using recipe equivalent to the previous formula}

simple_ames <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames_train) %>% 
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_dummy(all_nominal_predictors())
simple_ames

```

Let's break this down:

1 The call to `recipe()` with a formula tells recipe the  *roles* of the "ingredients" or variables (e.g. predictor, outcome). It only uses the data `ames_train` to determine the data types for the columns.

2. `step_log()` declares that `Gr_Liv_Area` should be log transformed.

3. `step_dummy()` specifies which variables should be converted from a qualitative format to a quantitative format, in this case, using dummy or indicator variables.

The function `all_nominal_predictors` captures the names of any predictor columns that are currently factor of character (i.e. nominal) in nature. This is a **dplyr**-like selector function similar to `starts_with()` or `matches()` but that can only be used inside of a recipe.

What is the advantage to using a recipe, over a formula or raw predictors? There are a few including:

* These computations can be recycled across models since they are not tightly coupled to the modeling function.

* A recipe enables a broader set of data processing choices than formulas can offer.

* The syntax can be very compact. For example `all_nominal_predictors()` can be used to capture many variables for specific types of processing while the formula would require each to be explicitly listed.

* All data processing can be captured in a single R object instead of scripts that are repeated or even spread across different files.

## 8.2 Using recipes

As we discussed in Chapter 7, preprocessing choices and feature engineering should typically be considered part of a modeling workflow, not a separate task. The **workflows** package contains high level functions to handle different types of preprocessors. Our previous workflow (`lm_workflow`) You stay simple set of **dplyr** selectors. To improve on that approach with more complex featuring engineering, let's use the `simple_ames` recipe to pre-process data for modeling.

We can only have one pre-processing method at a time, so we need to remove the existing pre-processor before adding the recipe.

```{r Attach simple_ames to the workflow}

lm_workflow <- 
  lm_workflow %>% 
  remove_variables() %>% 
  add_recipe(simple_ames)
lm_workflow

```


Let's estimate both the recipe and model using a simple call to `fit()`:

```{r Estimate both the recipe and model using a simple call to fit()}

lm_fit <- fit(lm_workflow, ames_train)

```

The `predict()` method applies the same preprocessing that was used on the training set to the new data before passing them along to the model's `predict()` method:

```{r Using the predict() method}

predict(object = lm_fit, new_data = ames_test %>% slice(1:3))

```

If we need the bare model object or recipe, there are `extract_*` functions that can retrieve them:

```{r Get the recipe after it has been estimated}

lm_fit %>% 
  extract_recipe(estimated = TRUE)

```

This returns the parsnip object:

```{r Returns the parsnip object}

lm_fit %>% 
  extract_fit_parsnip() %>% 
  tidy() %>% 
  slice(1:5)

```

## 8.2 How Data Are Used by the `recipe()`

Data are passed to recipes at different stages.

First, when calling `recipe(..., data)`, the data set is used to determine the data types of each column so that selectors such as `all_muneric()` or `all_numeric_predictors()` can be used.

Second, when preparing the data using `fit(workflow, data)`, the training data are used for all estimation operations including a recipe that may be part of the `workflow`, from determining factor levels to computing PCA components and everything in-between.

Finally, when using `predict(workflow, new_data)`, no model or preprocessor parameters like those from recipes are re-estimated using the values in `new_data`.

## Examples of recipe Sets

Before proceeding, let's take an extended tour of the capabilities of **recipes** and explore some of the most important`step_*()` functions. These recipe step functions each specify a specific possible step in a feature engineering process, and different recipe steps can have different effects on columns of data.

### 8.4.1 Encoding Qualitative Data In A Numeric Format

One of the most common feature engineering tasks is transforming nominal or qualitative data (factors or characters) so they can be encoded or represented numerically. Sometimes we can alter the factor levels of a qualitative column in helpful ways prior to such a transformation. For example, `step_unknown()` can be used to change missing values to a dedicated factor level. Similarly, if we anticipate that a new factor level may be encountered in future data, `step_novel()` can allot a new level for this purpose.

Additionally, `step_other()` can be used to analyze the frequencies of the factor levels in the training set and convert infrequently occurring values to a catch-all level of "other", with a threshold that can be specified. A good example is the `Neighborhood` predictor in our data, shown in Figure 8.1: 

```{r barchart of neighborhoods in the Ames training set}

ggplot(ames_train, aes(x = Neighborhood)) +
  geom_bar() +
  coord_flip()

```

Here we see the two neighborhoods have less than five properties and the training data (Landmark and Green Hills); in this case, no houses at all in the Landmark neighborhoods were included in the training set. For some models, it may be problematic to have dummy variables was single nonzero entries in the column. At a minimum, it is highly improbable that these features would be important to a model. If we add `step_other(Neighborhood, threshold = 0.01)` to our recipe, the bottom 1% of the neighborhoods will be lumped into a new level called "other." In this training set, this will catch seven neighborhoods.

From the Ames data, we can amend the recipe to use:

```{r Amended Ames training set to group lowest number of properties in neighborhood}

simple_ames <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames_train) %>% 
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors())

```

The most common method for converting a factor predictor to a numeric format is to create dummy or indicator variables. Let's take the predictor in the Ames data for the building type, which is a factor variable with five levels (see table 8.1). For dummy variables, the single ` Bldg_Type` column would be replaced with four numeric columns whose values are either zero or one. These binary variables represent specific factor levels values. In R, the convention is to exclude a column for the first factor level (`OneFam`, in this case). The `Bldg_Type` column would be replaced with a column called `TwoFmCon` that is one when the row has that value, and zero otherwise. Three other columns are similarly created:

<center>Table 8.1: Illustration of binary encodings (i.e. dummy variables) for a qualitative predictors</center>

||
|:--|:--|:--|:--|:--|
|**Raw Data**|**TwoFmCon**|**Duplex**|**Twnhs**|**TwnhsE**|
|OneFam|0|0|0|0|
|TwoFmCon|1|0|0|0|
|Duplex|0|1|0|0|
|Twnhs|0|0|1|0|
|TwnhsE|0|0|0|1|
---

The full set of encodings can be used for some models. This is traditionally called one-hot encoding and can be achieved using the `one_hot` argument of `step_dummy()`.

Recipes, by default, using_as a separator between name and level (e.g. `Neighborhood_Veenker`) and there is an option to use custom formatting for the names. The default naming convention in **recipes** makes it easier to capture those new columns in future steps using a selector, such as `starts_with("Neighborhood_")`

* *Feature hashing* methods only consider the value of the category to assign it to a predefined pool of dummy variables.
* *Effect* or *likelihood encodings* replace the original data with a single numeric column that measures the *effect* of those data.

Both feature hashing and effect encoding can seamlessly handle situations where a novel factor level is encountered in the data. Chapter 17 explores these and other methods for encoding categorical data, beyond straightforward dummy or indicator variables.

### 8.4.2 Interaction terms

Numerically, an interaction term between predictors is encoded as their product. Interactions are derived in terms of their effect on the outcome and can be combinations of different types of data (e.g. numeric, categorical, etc).

After exploring the Ames training set, we might find that the regression slopes for the gross living area differ for different building types, as shown in Figure 8.2

```{r Regression lines of Sale Price vs building type in the Ames data set}

ggplot(ames_train, aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = 0.2) +
  facet_wrap(~Bldg_Type) +
  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "lightblue") +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "Gross Living Area", y = "Sale Price (USD)")

```

Recipes are more explicit and sequential (than traditional formula methods) and they give you more control. With the current recipe, `step_dummy()` has already created dummy variables. How would we combine these for an interaction? The additional step would look like `step_interact(~ interaction terms)` where the terms on the right-hand side of the tilde are the interactions. These can include selectors, so it would be appropriate to use:

```{r Example of interaction terms}

simple_ames <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames_train) %>% 
  step_log(Gr_Liv_area, base= 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_area:starts_with("Bldg_Type_") )

tidy(simple_ames) # to see the steps in the recipe

lm_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

```

To see the steps in the recipe:

```{r To see the steps in the recipe}

tidy(simple_ames)

```

To see the underlying model:

```{r to see the underlying model}

lm_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

```


Additional interactions can be specified in this formula by separating them with `+`.

As with naming dummy variables, **recipes** provides more coherent names for interaction terms.In this case, the interaction is named `Gr_Liv_Area_x_Bldg_Type_Duplex` instead of `Gr_Liv_area:Bldg_TypeDuplex` (which is not a valid column name for a data frame).

*Remember that order matters.* The Gross Living Wage is log transformed prior to the interaction term. Subsequent interactions with this variable will also use the log scale.

### 8.4.3 Spline Functions

One common method for fitting a data set is to use *spline* functions to represent the data. Splines replace the existing predictor with a set of columns that allow a model to emulate a flexible, nonlinear relationship.As more spline terms are added to the data, the capacity to nonlinearly represent the relationship increases. Unfortunately, it may also increase the likelihood of picking up on data trends that occur by chance (i.e., overfitting).

If you have ever used `geom_smooth()` within a `ggplot`, we have probably used a spline representation of the data. For example, each panel in figure 8.3 uses a different number of smooth lines for the latitude predictor.

```{r Examples of splines}

plot_smoother <- function(deg_free){
  ggplot(ames_train, aes(x = Latitude, y = 10^Sale_Price)) +
    geom_point(alpha = 0.2) +
    scale_y_log10() +
    geom_smooth(
      method = lm,
      formula = y ~ ns(x, df = deg_free),
      color = "lightblue",
      se = FALSE
    ) +
    labs(title = paste(deg_free, "Spline Terms"),
         y = "Sale Price (USD)")
}

(plot_smoother(2) + plot_smoother(5)) / (plot_smoother(20) + plot_smoother(100))
```

The `ns()` function in the **splines** package generates feature columns using functions called *natural splines.*

Some panels in figure 8. Three clearly fit poorly; two terms *underfit* the data while 100 terms *overfit*. The panels with five and 20 terms seem like reasonably smooth fits that catch the main patterns of the data. This indicates that the proper amount of "nonlinear–ness" matters. The number of spline terms could be considered a *tuning parameter* for this model. These types of parameters are explored in chapter 12.

In **recipes** multiple steps can create these types of terms. To add a natural spline representation for this predictor:

```{r Adding a natural spline for the sale price predictor}

recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude, data = ames_train) %>% 
  step_log(Gr_Liv_area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_area:starts_with("Bldg_type_") ) %>% 
  step_ns(Latitude, deg_free = 20)

```

### 8.4.4 Feature Extraction

Another common method for representing multiple features is called*feature extraction*. Most of these techniques create new features from the predictors that capture the information in the broader set as a whole. For example, principal components analysis (PCA) tries to capture as much of the original information in the predictor set as possible using a smaller number of features. PCA is a linear extraction method, meaning that each new feature is a linear combination of the original predictors. One nice aspect of PCA is that each of the new features, called the principal components or PCA scores, are uncorrelated with one another. Because of this, PCA can be very effective at reducing the correlation between predictors. Note that PCA is only aware of the predictors; the new PCA features might not be associated with the outcome.

There are existing recipe steps for other extraction methods, such as: Independent component analysis (ICA), non-negative matrix factorization(NNMF), multidimensional scaling (MDS), uniform manifold approximation and projection (UMAP) and others.

### 8.4.5 Row Sampling Sets

Recipe steps can affect the rose of a data set as well. For example, *subsampling* techniques for class and balances changes the class proportions in the data being given to the model; these techniques often don't improve overall performance but can generate better behaved distributions of the predicted class probabilities. These are approaches to try one subsampling your data with class and balance:

**Down sampling*the data keeps the minority class and takes a random sample of the majority class so that class frequencies are balanced.

**Up sampling*replicates samples from the minority class to balance the classes. Some techniques do this by synthesizing new samples that resemble the minored class data while other methods simply add the same minority samples repeatedly.

* *Hybrid methods* Do a combination of both.

The**famous**package as recipe steps that can be used to address classic balance via subsampling. For simple down sampling we would use:

```{r How to use downsampling}

# step_downsample(outcome_column_name)

```

Other step functions are row-based as well: `step_filter()`, `step_sample()`, `step_slice()`, and `step_arrange()`. In almost all uses of these steps, the `skip` argument should be set to `TRUE`.

### 8.4.7 Natural Language Processing

Recipes can also handle data that are not in the traditional structure where the columns are features. For example, the **textrecipes** package can apply natural language processing methods to the data. The input column is typically a string of text, and different steps can be used to tokenize the data (e.g. split the text into separate words), filter out tokens, and create new features appropriate for modeling.

