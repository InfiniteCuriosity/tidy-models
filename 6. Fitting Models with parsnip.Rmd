---
title: "6. Fitting Models with parsnip"
author: "Russ Conte"
date: '2022-05-26'
output: html_document
---


Start by loading the required packages in two lines of code:

```{r Load all required packages in two lines of code}

Packages <- c("tidymodels", "caret")
lapply(Packages, library, character = TRUE)

tidymodels_prefer(quiet = FALSE)

```

Start with code for multi-core processing

```{r multi-core processing}

library(doParallel)
cl <- makePSOCKcluster(10)
registerDoParallel(cl)

```

The **parsnip** package, one of the R packages that are part of the **tidymodels** metapackage, provides a fluent and standardized interface for a variety of different models. In this chapter, we give some motivation for why a common interface is beneficial for understanding and building models in practice and show how to use the **parsnip** package.

6.1 Create a Model

Suppose that a linear regression model was our initial choice. This is equivalent to specifying that the outcome data is numeric and that the predictors are related to the outcome in terms of simple slopes and intercepts:

<center>$y_i = \beta_{0} + \beta_{1}x_{1i} + ... + \beta{_p}x_{pi}$</center>

A variety of methods can be used to estimate the model parameters:

* *Ordinary linear regression* uses the traditional method of least squares to solve for the model parameters.

* *Regularized linear regression* adds a penalty to the least squares method to encourage simplicity by removing predictors and/or shrinking their coefficients towards zero. This can be executed using Bayesian or non-Bayesian techniques.

In R, the **stats** package can be used for the first case. The syntax for linear regression using the function `lm()` is:

```{r Example of linear regression function}

#model <- lm(formula, data, ...)

```

To estimate with regularization, the second case, a Bayesian model can be fit using the **rstanarm** package:

```{r Fitting a regularized model}

#model <- stan_glm(formula, data, family = "gaussian", ...)

```

A popular non-Bayesian approach to regularized regression is the **glmnet** model (Friedman, Hastie, and Tibshirani 2010), Its syntax is:

```{r glmnet model}

#model <- glmnet(x = matrix, y = vector, family = "gaussian")

```

In this case, the predictor data must already be formatted into a numeric matrix; there is only an `x`/`y` method and no formula method.

For tidymodels, the approach to specifying a model is intended to be more unified:

1. *Specify the model based on its mathematical structure* (e.g. linear regression, random forest, KNN, etc.)
2. *Specify the engine for fitting the model*. Most often this reflects the software package that should be used, like Stan or **glmnet**. These are models in their own right, and **parsnip** provides consistent interfaces by using these as engines for modeling.

3. *When required, declare the mode of the model*. The mode reflects the type of prediction outcome. For numeric outcomes, the mode is regression. For qualitative outcomes, it is classification. If a model algorithm can only address one type of prediction outcome, such as linear regression, the mode is already set.

These specifications are built without referencing the data.

Once the details of the model have been specified, the model estimation can be done wit either the `fit()` function (to use a formula) or the `fit_xy()` function (when your data are already pre-processed). The **parsnip** package allows the user to be indifferent to the interface of the underlying model you can always use a formula even if the modeling package's function only has the `x`/`y` interface.

The `translate()` function can provide details on how **parsnip** converts the user's code to the package's syntax:

Linear Regression Model Specifications:
Computation engine is: lm
Model fit template: stats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())

```{r Example of translate function applied to linear regression}

linear_reg() %>% set_engine("lm") %>% translate()

```

Penalized Linear Regression Model Specifications:
Main arguments: Penalty = 1
Computational engine: glmnet
Model fit template: glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg, family = "gaussian)

```{r Example of penalized regression}

linear_reg(penalty = 1) %>% set_engine("glmnet") %>% translate()

```

Let us walk through how to predict the sale price of properties in the Ames housing data as a function of only longitude and latitude:

```{r Fitting the ames property data as only a function of longitude and latitude}

set.seed(502)
data(ames)
ames <- ames %>% mutate(Sale_Price = log10(Sale_Price))
ames_split <- initial_split(ames, prop = 0.8, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)

lm_model <-
  linear_reg() %>%
  set_engine("lm")

lm_form_fit <-
  lm_model %>%
  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)

lm_xy_fit <- lm_model %>%
  fit_xy(
    x = ames_train %>% select(Longitude, Latitude),
    y = ames_train %>% pull(Sale_Price)
  )

lm_form_fit

lm_xy_fit

```

Not only does **parsnip** enable a consistent model interface for different packages, it also provides consistency in the model arguments. It is common for different functions that fit the same model to have different argument names. Random Forest model functions are a good example. Three commonly used arguments are the number of trees in the ensemble, the number of protectors randomly sample with each split within a tree, and the number of data points required to make a split. For three different R packages mining this algorithm, the arguments are shown in table 6.1.


<center><b>Table 6.1: Example argument names for different random forest function.</b></center>
||
|:-----|:-----|:-----|:-----|
|**Argument Type**|**ranger**|**randomForest**|**sparklyr**|
|# sampled predictors|mtry| mtry|feature_subset_strategy|
|# trees|num.trees|ntree|num_trees|
|# data points to split|min.note.size|nodesize|min_instances_per_node|
___

In an effort to make argument specification less painful, **parsnip** uses common argument names within and between packages. Table 6.2 shows, for random forests, what **parsnip** does:

<center>Table 6.2: Random forest argument names used by parsnip</center>
||
|:---|:---|
|**Argument Type**|**parsnip**|
|# sampled predictors|mtry|
|# trees|trees|
|# data points to split|min_n|
---

Admittedly, this is one more set of arguments to memorize. However, when other types of models have the same argument types, these names still apply. For example, boosted three ensembles also create a large number of three-based models, so `trees` is also used there, as is `min_n`, and so on.

Our rule of thumb when standardizing argument names is:

>If a practioner were to include these names in a plot or table, would the people viewing those results understand the name?

To you understand how the **parsnip** argument names map to the original names, use the help file for the model (available via `?rand_forest`) as well as the `translate` function:

```{r Understanding how parsnip names map to the original names}

rand_forest(trees = 1000, min_n = 5) %>% 
  set_engine("ranger") %>% 
  set_mode("regression") %>% 
  translate()

```

Modeling functions in **parsnip** separate model arguments into two categories:

* *Main arguments* are more commonly used and tend to be available across engines.
* *Engine arguments* are either specific to a particular engine or used more rarely.

## 6.2 Use the Model Results

Once the model is created and fit, we can use the results in a variety of ways; we might want to plot, print, or otherwise examine the model output. Several quantities are stored in a **parsnip** model object, including the fitted model. This can be found in an element called `fit`, which can be returned using the `extract_fit_engine()` function:

```{r Using the model results}

lm_form_fit %>% extract_fit_engine()

```

Normal methods can be applied to this object, such as printing and plotting:

```{r Printing and plotting a parsnip object}

lm_form_fit %>% extract_fit_engine() %>% vcov()

```

The **broom** package can convert many types of model objects to a tidy structure. For example, using the `tidy()` method on the linear model produces:

```{r Using the tidy() method on the linear model}

tidy(lm_form_fit)

```

One important principle in the tidymodels ecosystem is that a function should return values that are *predictable, consistent,* and *unsurprising*.

## 6.3 Make Predictions

Another area where**Parsnip**diverges from conventional are modeling functions is the format a values return from `predict()`. For predictions, parsnip always conforms to the following rules:

1. The results are always a tibble.

2. The column names of the tibble are always predictable.

3. There are always as many rows in the table is there are in the input data set.

For example when numeric data are predicted:

```{r Example using parsnip when numeric data are predicted}

ames_test_small <- ames_test %>% slice(1:5)
predict(lm_form_fit, new_data = ames_test_small)

```

Note that the row order of the predictions are always the same as the original data.

These three rules make it (much) easier to merge predictions with the original data, including a 95% confidence interval of the log of the sale price:

```{r Applying three rules to make it much easier to merge the original data with predicitons}

ames_test_small %>% 
  select(Sale_Price) %>% 
  bind_cols(predict(lm_form_fit, ames_test_small)) %>% 
  bind_cols(predict(lm_form_fit, ames_test_small, type = "pred_int"))

```

For the second tidymodels prediction rule, the predictable column names for different types of predictions are shown in Table 6.4:

<center>Table 6.4: The tidymodels mapping of prediction types and column names</center>
||
|:---|:---|
|**type value**|**column name(s)**|
|numeric|.pred|
|class|.pred_class|
|prob|.pred_{class levels|
|conf_int|.pred_lower, .pred_upper|
|pred_int|.pred_lower, .pred_upper|
---

The third rule regarding the number of rows in the output is critical. For example, if any rows of the new data contain missing values, the output will be padded with missing results for those rows. A main advantage of standardizing the model interface and prediction types in **parsnip** is that, one different model are used, the syntax identical.Suppose that we used a decision tree to model the aims data. Outside of the model specification, there are no significant differences in the code pipeline:

```{r Demonstrating consistency in the number of rows in the output}

tree_model <- 
  decision_tree(min_n = 2) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

tree_fit <- 
  tree_model %>% 
  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)

ames_test_small %>% 
  select(Sale_Price) %>% 
  bind_cols(predict(tree_fit, ames_test_small))
```

This demonstrates the benefit of homogenizing the data analysis process in syntax across different models. It enables users to spend their time on the results and interpretation rather than having to spend time on the syntactical differences between R packages.

## 6.4 Parsnip-Extension Packages

The **parsnip** package itself contains interfaces to a number of models. However, for ease of package installation and maintenance, there are other tidy model packages that have **parsnip** model definitions for other sets of models. The **discrim** package has model functions for the set of classification techniques called discriminant analysis models, such as linear or quadratic discriminant analysis. In this way, the package dependencies required for installing **parsnip** are reduced. A list of all of the models that can be used with **parsnip** (across different packages that are on CREN) can be found out: <https://www.tidymodels.org/find>

## 6.5 Creating Model Specifications

It may become tedious to write many model specifications, or to remember how to write the code to generate them. The **parsnip** package includes an RStudio addin that can help. Either choosing this ad in from the*Adams*two bar menu are running the code:

```{r Running the code to access the parsnip addins}

#parsnip_addin()

```

## 6.6. Chapter Summary

This chapter introduced the **parsnip** package, which provides a common interface from models across R packages using a standard syntax. The interface and resulting objects have a predictable structure.

The code for modeling the AIM stated that we will use moving forward is:

```{r Code we will use for modeling the Ames data moving forward}

library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_rpcie = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.8, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)

lm_model <- linear_reg() %>% set_engine("lm")

```

REFERENCES

Friedman, J, T Hastie, and R Tibshirani. 2010. “Regularization Paths for Generalized Linear Models via Coordinate Descent.” Journal of Statistical Software 33 (1): 1.