---
title: "10. Resampling for Evaluating Performance"
author: "Russ Conte"
date: '2022-05-27'
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Start by loading the required packages in two lines of code:

```{r Load all required packages in two lines of code}

Packages <- c("tidymodels", "caret", "multilevelmod", "lme4", "VCA", "survival", "patchwork", "splines", "ranger")
lapply(Packages, library, character = TRUE)

tidymodels_prefer(quiet = FALSE)

```

Start with code for multi-core processing

```{r multi-core processing}

library(doParallel)
cl <- makePSOCKcluster(10)
registerDoParallel(cl)

```

```{r Code for this chapter, from previous chapters}

library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)
  
lm_model <- linear_reg() %>% set_engine("lm")

lm_workflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_workflow, ames_train)

```

## 10.1 The Resubstitution Approach

One way measure performance on the same data that we used for training
(as opposed to new data or testing data), we say we have *resubstituted*
the data. Let's again use the Ames housing data to demonstrate these
concepts. Section 8.8 summarizes the current state of our aims analysis.
It includes a recipe object named `Ames_rec`,a linear model, and a
workflow using that recipe called `lm_workflow`. This workflow was fit
on the training set, resulting in `lm_fit`.

For a comparison to this linear model, we can also fit a different type
of model.*Random forests* are a tree ensemble method that operates by
creating a large number of decision trees from slightly different
versions of the training set. This collection of trees makes up the
ensemble. When predicting a new sample, each ensemble member makes a
separate prediction. These are averaged to create the final on summer
prediction for the new data point.

Random forest models are very powerful, and they can emulate the
underlying data patterns very closely. While this model can be
computationally intensive, it is very low maintenance; very little
pre-processing as required (as documented in appendix A).

Using the same predictor set as the linear model (without the extra
preprocessing steps), we can fit a random forest model to the training
set via the `"ranger"` engine (which uses the **ranger** R package for
computation. This model requires no pre-processing, so so a simple
formula can be used:

```{r Random Forest model with the Ames data set}

rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_workflow <- 
  workflow() %>% 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude + Longitude) %>% 
  add_model(rf_model)

rf_fit <- rf_workflow %>% fit(data = ames_train)

rf_fit
```

How should we compare the linear and random forest models? For
demonstration, we will predict the training set to produce what is known
as an *apparent metric* or *resubstitution metric*. This function
creates predictions and formats the results:

```{r Creating an apparent metric or resubstitution metric}

estimate_perf <- function(model, dat){
  # Capture the names of the `model` and `dat` objects
  cl <- match.call()
  obj_name <- as.character(cl$model)
  data_name <- as.character(cl$dat)
  data_name <- gsub("ames_", "", data_name)
  
  # Estimate the metrics:
  reg_metrics <- metric_set(rmse, rsq)
  
  model %>% 
    predict(dat) %>% 
    bind_cols(dat %>% select(Sale_Price)) %>% 
    reg_metrics(Sale_Price, .pred) %>% 
    select(-.estimator) %>% 
    mutate(object = obj_name, data = data_name)
  }

```

Both RMSE and $R^2$ are computed. The resubstituion statistics are:

```{r RMSE and resubstitution metrics for the random forest model of the Ames data set}

estimate_perf(rf_fit, ames_train)

estimate_perf(lm_fit, ames_train)
```

Based on these results, the random forest is much more capable of
predicting the sales prices, the RMSE estimate is two fold better than
linear regression. If we needed to choose between these two models for
this price prediction problem, we will probably choose the random
forest, on a log scale we are using, it's RMSE is about half as large.
The next step applies the random forest model to the test set for final
verification:

```{r apply random forest model to the test set}

estimate_perf(rf_fit, ames_test)

```

The test RMSE estimate, 0.0704, is *much worse than the training set*
value of 0.0365! Why did this happen?

Many predictive models are capable of learning complex trends from the
data. In statistics, these re commonly referred to as *low bias models.*

In this context, *bias* is the difference between the true pattern or
relationship in data and the types of patterns that the model can
emulate. Many black-box machine learning models have low bias, meaning
they can reproduce complex relationships. Other models (such as
linear/logistic regression, discriminant analysis, and others) are not
as adaptable and are considered *high bias* models.

For a low bias model, the high degree of predictive capacity can
sometimes result in the model nearly memorizing the training set data.
Random forests are one example where this can happen. repredicting the
training set will **always** result in an artificially optimistic
estimate of model performance.

For both models, table 10.1 summarizes the RMSE estimate for the
training and test sets:


<center>Table 10.1: Performance statistics for training and test sets</center>
||
|:--|:--|:--|
||**RMSE**|**Estimates**|
|**object**|**train**|**test**|
|lm_fit|0.0754|0.0736|
|rf_fit|0.0365|0.0704|
---

The main takeaway from this example is that repredicting the training
set will result in an artificially optimistic estimate of performance.
It is a bad idea for most models.

If the test set should not be used immediately, and repredicting the
training set is a bad idea, what should be done? Resampling methods,
such as cross-validation or validation sets, are the solution.

## 10.2 Resamplling Methoes

Most sampling methods are empirical simulation systems that emulate the
process of using some data for modeling and different data for
evaluation.Most resampling methods are iterative, meaning that this
process is repeated multiple times.

Resembling is conducted only on the training set, the test said is not
involved. For each adoration of sampling, the data are partitioned into
two sub samples:

-   the model is fit with the *analysis set*.

-   the model is evaluated with the *assessment set*.

These two sub samples are somewhat analogous to training and test
sets.Our language of *analysis* and *assessment* avoids confusion with
the initial split of the data. These data sets are mutually exclusive.
The partitioning scheme used to create analysis in assessment sets is
usually the defining characteristic of the method.

The next section defines several commonly used resampling methods and
discusses their pros and cons.

### 10.2.1 Cross-Validation

Cross-validation is a well established resampling method. While there
are a number of variations, the most common cross validation method is
V--fold (such as 10-fold cross-validation). For example, when V=3, the
analysis sets are 2/3 of the training set and each assessments that is a
distinct 1/3. The final resampling estimate of the performance averages
each of the V replicates.

In practice, values of V the are most often 5 or 10; we generally prefer
10--fold cross--validation as a default because it is large enough for
good results in most situations.

The primary input is the training set data frame as well as the number
of folds (defaulting to 10):

```{r First example in this book of cross-validation}

set.seed(1001)
ames_folds <- vfold_cv(ames_train, v = 10)
ames_folds


```

The column named `splits` contains information and how to split the data
(similar to the object used to create initial training/test partition).
While each row of `splits` has an embedded copy of the entire training
site, R a smart enough not to make copies of the data in memory. The
print method inside of the tibble shows the frequency of each:
`[2k/220]` Indicates that roughly 2000 samples are in the analysis set
and 220 are in that particular assessment set.

These optics also always contain a character column called back tick ID
back tick that labels the partition.

To manually retrieve the partitioned data, the `analysis()` and
`assessment()` functions return the corresponding data frames:

```{r Manually retreive the partitioned data}

# for the first fold:
ames_folds$splits[[1]] %>% 
  analysis() %>% 
  dim()

```

The **tidymodels** packages, such as **tune**, contain high-level user
interfaces to that functions like `analysis()` are generally not needed
for day-to-day work. Section 10.3 demonstrates a function to fit a model
over these resamples.

There are a variety of cross-validation variations; we'll go through the
most important ones.

The most important variation on cross-Validation is repeated V-fault
cross validation. Depending on data size or other characteristics, the
resampling estimate produced by V-fault cross validation may be
excessively noisy.As with many statistical problems, one way to reduce
noise is to gather more data. For cross -- validation, this means
averaging more than V statistics.

Large numbers of replicates tend to have less impact on the standard
error. However, if the baseline value of $\sigma$ is impractically
large, the diminishing returns on replication may still be worth the
extra computational costs.

To create repeats, invoke `vfold_cv()` with an additional argument
`repeats`:

```{r 10-fold cross validation}

vfold_cv(ames_train, v = 10, repeats = 5)

```

### Monte Carlo Cross-Validation

Another variant of re-fold cross validation is Monte Carlo cross
validation (MCCV, Xu and Liang (2001)). Like V -- fold cross validation,
it allocates a fixed proportion of data to the assessment sets. The
difference between MCCV and regular cross-validation is that, for MCCV,
this proportion of the data is randomly selected each time. To create
this resembling objects:

```{r Monte-Carlo Cross Validation}

mc_cv(ames_train, prop = 9/10, times = 20)

```

## 10.2.2 Validation Sets

When using a validation set, the initial available data set is split
into a training set, validation set, and a test set.

Validation sets are often used when the original pool of data is very
large. In this case, a single large partition may be adequate to
characterize model performance without having to do multiple resampling
iterations.

With the **rsample** package, a validation set is like any other
resembling object; this type is different only in that it has a single
iteration.

To create a validation set object that uses 3/4 of the data for model
fitting:

```{r Create a validation set that uses 75% of the data for model fitting}

set.seed(1002)
val_set <- validation_split(ames_train, prop = 3/4)
val_set

```

### 10.2.3 Bootstrapping

Bootstrap resembling was originally invented as a method for approximating the sampling distribution of statistics who theoretical properties are intractable (Davison and Hinckley 1997). Using bootstrapping to estimate model performance as a secondary application of the method.

Using the **rsample** package, we can create such bootstrap resamples:

```{r Bootstrap the Ames data set}

bootstraps(ames_train, times = 5)

```

Bootstrap samples produce performance estimates that have very variance (unlike cross-validation) but have significant pessimistic bias. This means that if the true accuracy of a model is 90%, the bootstrap would tend to estimate the value to be less than 90%.

3310.2.4 Rolling forecasting original resampling

Rolling forecast orange in resampling (Hyndman, R, and G Athanasopoulos) provides a method that relates how timeseries data is often partitioned in practice, estimating the model with historical data and evaluating it with the most recent data. For a year's worth of data, suppose that six sets of 30–day blocks define the analysis set. For assessment sets of 30 days with a 29–day skip, we can use the **rsample** package to specify:

```{r Rolling forecasting origina resampling}

time_slices <- 
  tibble(x = 1:365) %>% 
  rolling_origin(initial = 6 * 30, assess = 30, skip = 29, cumulative = FALSE)

data_range <- function(x){
  summarise(x, fist = min(x), last = max(x))
}

map_dfr(time_slices$splits, ~ anslysis(.x) %>% data_range())

map_dfr(time_slices$splits, ~ assessment(.x) %>%  data_range())

```

